{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d36bef-1353-44a0-95ef-4cb8b98a2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbe0973-27e3-4934-b54b-f611678bf9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, root_dir1, root_dir2, transform=None):\n",
    "        self.root_dir1 = root_dir1\n",
    "        self.root_dir2 = root_dir2\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # 두 디렉토리 안의 이미지 파일을 쌍으로 찾기\n",
    "        for filename in os.listdir(root_dir1):\n",
    "            img1_path = os.path.join(root_dir1, filename)\n",
    "            img2_path = os.path.join(root_dir2, filename)\n",
    "            if os.path.isfile(img1_path) and os.path.isfile(img2_path):\n",
    "                # 올바른 쌍인 경우 레이블을 0으로 설정\n",
    "                self.samples.append((img1_path, img2_path, 0))\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path, label = self.samples[idx]\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        \n",
    "        return img1, img2, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbd613e-90d0-40d5-9d5b-7535f9dd92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrongImagePairDataset(Dataset):\n",
    "    def __init__(self, root_dir1, root_dir2, transform=None):\n",
    "        self.root_dir1 = root_dir1\n",
    "        self.root_dir2 = root_dir2\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # 두 디렉토리 안의 이미지 파일을 쌍으로 찾기\n",
    "        for filename in os.listdir(root_dir1):\n",
    "            img1_path = os.path.join(root_dir1, filename)\n",
    "            img2_path = os.path.join(root_dir2, filename)\n",
    "            if os.path.isfile(img1_path) and os.path.isfile(img2_path):\n",
    "                # 올바른 쌍인 경우 레이블을 1으로 설정\n",
    "                self.samples.append((img1_path, img2_path, 1))\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path, label = self.samples[idx]\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        \n",
    "        return img1, img2, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bf335c-4e56-4a54-913b-6266acbca692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedImagePairDataset(Dataset):\n",
    "    def __init__(self, dataset1, dataset2):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.total_length = len(dataset1) + len(dataset2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.dataset1):\n",
    "            return self.dataset1[idx]\n",
    "        else:\n",
    "            # Adjust the index to fit within the second dataset\n",
    "            adjusted_idx = idx - len(self.dataset1)\n",
    "            return self.dataset2[adjusted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba54139d-61a6-4895-ba94-4f3decb163cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageComparisonModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ImageComparisonModel, self).__init__()\n",
    "        \n",
    "#         # Stream 1 for the first image modality\n",
    "#         self.stream1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         # Stream 2 for the second image modality\n",
    "#         self.stream2 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         # Fusion and decision mechanism\n",
    "#         self.fusion = nn.Sequential(\n",
    "#             nn.Linear(32*32*128*2, 1024),  # Assuming images are 224x224 size, adjust if different\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(512, 1)  # Binary classification\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x1, x2):\n",
    "#         x1 = self.stream1(x1)\n",
    "#         x2 = self.stream2(x2)\n",
    "        \n",
    "#         # Flatten the features from both streams\n",
    "#         x1 = x1.view(x1.size(0), -1)\n",
    "#         x2 = x2.view(x2.size(0), -1)\n",
    "        \n",
    "#         # Concatenate the features\n",
    "#         x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "#         x = self.fusion(x)  # 수정된 부분: fusion 레이어 추가\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4bba5a-245e-4524-bfeb-4bebfa4982d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion 레이어:\n",
    "\n",
    "# Fusion 레이어는 두 개의 입력을 결합하고 합쳐진 특징을 생성하는 역할을 합니다. \n",
    "# 이 레이어는 모델이 입력 데이터를 효과적으로 처리하고 결합하는 데 중요한 역할을 합니다. \n",
    "# 따라서 Fusion 레이어가 입력 데이터를 잘 결합하여 모델의 성능을 향상시키는 경우, 이를 사용하는 것이 좋습니다.\n",
    "\n",
    "# Comparison 레이어: \n",
    "\n",
    "# Comparison 레이어는 결합된 특징을 사용하여 최종 예측을 생성하는 역할을 합니다. \n",
    "# 이 레이어는 모델이 입력 데이터의 특징을 비교하고 결합하여 원하는 작업을 수행하는 데 중요한 역할을 합니다. \n",
    "# 따라서 Comparison 레이어가 효과적인 예측을 생성하는 경우, 이를 사용하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b1447e-dacb-4fb2-ba7c-254f7b5ab5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageComparisonModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageComparisonModel, self).__init__()\n",
    "        \n",
    "        # Stream 1 for the first image modality\n",
    "        self.stream1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Stream 2 for the second image modality\n",
    "        self.stream2 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Fusion and decision mechanism\n",
    "        self.comparison = nn.Sequential(\n",
    "            nn.Linear(32*32*128*2, 1024),  # Assuming images are 224x224 size, adjust if different\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.stream1(x1)\n",
    "        x2 = self.stream2(x2)\n",
    "        \n",
    "        # Flatten the features from both streams\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        \n",
    "        # Concatenate the features\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Pass through the comparison mechanism\n",
    "        x = self.comparison(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ec69c8-9369-4a02-b01a-9bd3b0e5e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 초기화\n",
    "#combined_model = CombinedModel()\n",
    "combined_model = ImageComparisonModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f355496-7990-4d70-ad51-2feed76eb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # 이미지 크기를 64x64로 조정\n",
    "    transforms.ToTensor(),         # 이미지를 텐서로 변환\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 정규화\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e555f6-8a7e-48f1-9451-8f635f9895d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67ae6feb-b3d6-4cd9-96e0-94b418478250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "\n",
    "chemdraw_train = \"data/train/chemdraw_train\"\n",
    "chemdraw_test = \"data/test/chemdraw_test\"\n",
    "chemdraw_valid = \"data/valid/chemdraw_val\"\n",
    "\n",
    "chemdraw_train_smlies = \"data/train/chemdraw_train_smlies\"\n",
    "chemdraw_test_smlies = \"data/test/chemdraw_test_smlies\"\n",
    "chemdraw_valid_smlies = \"data/valid/chemdraw_val_smlies\"\n",
    "\n",
    "chemdraw_train_smlies_wrong = \"data/train/chemdraw_train_smlies_wrong\"\n",
    "chemdraw_test_smlies_wrong = \"data/test/chemdraw_test_smlies_wrong\"\n",
    "chemdraw_valid_smlies_wrong = \"data/valid/chemdraw_val_smlies_wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a203eea8-5920-424b-b0fd-abffe7b5bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 생성\n",
    "correct_train_dataset = ImagePairDataset(chemdraw_train, chemdraw_train_smlies, transform=transform)\n",
    "correct_test_dataset = ImagePairDataset(chemdraw_test, chemdraw_test_smlies, transform=transform)\n",
    "correct_valid_dataset = ImagePairDataset(chemdraw_valid, chemdraw_valid_smlies, transform=transform)\n",
    "\n",
    "wrong_train_dataset = WrongImagePairDataset(chemdraw_train, chemdraw_train_smlies_wrong, transform=transform)\n",
    "wrong_test_dataset = WrongImagePairDataset(chemdraw_test, chemdraw_test_smlies_wrong, transform=transform)\n",
    "wrong_valid_dataset = WrongImagePairDataset(chemdraw_valid, chemdraw_valid_smlies_wrong, transform=transform)\n",
    "\n",
    "\n",
    "combine_train_dataset = CombinedImagePairDataset(correct_train_dataset, wrong_train_dataset)\n",
    "combine_test_dataset = CombinedImagePairDataset(correct_test_dataset, wrong_test_dataset)\n",
    "combine_valid_dataset = CombinedImagePairDataset(correct_valid_dataset, wrong_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b3ef558-edae-4b6c-a622-ffb27d112ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(combine_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(combine_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(combine_valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385bd245-b78c-4289-831c-17f51bb90088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU를 사용할 수 있는지 확인합니다.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ImageComparisonModel().to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f20ad9d6-c533-4326-9d82-ffbf7c28d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: Contrastive Loss 직접 구현\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, target):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "        loss_contrastive = torch.mean((1 - target) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (target) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "697cd861-279e-4b12-8420-d71fd9efde46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 학습을 위한 하이퍼파라미터 설정\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "# 손실 함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "033d122f-2fe8-486d-804b-538842085d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 중간에 accuracy 프린트하는 부분 추가하기, 더 좋은 모델이 나왔을경우 모델을 저장하는 코드를 만들자.\n",
    "\n",
    "# 모델 코드는 새로운 폴더를 만들고 (if save else make and save) 현재 시간을 가져와 이름으로 +@ 해서 저장해주자.\n",
    "\n",
    "# 값이 더 이상 내려가지 않는다 -> 수렴한다, 높은 loss에서 수렴했다고 해서 나쁜 모델이 아니다. 만드시 정확도(accuracy)를 확인해주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3718bce-874c-42fd-aefe-b2a551c990bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs):\n",
    "#     train_losses = []\n",
    "#     valid_losses = []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_train_loss = 0.0\n",
    "        \n",
    "#         # 훈련 데이터로 모델 학습\n",
    "#         for inputs1, inputs2, labels in train_loader:\n",
    "#             inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs1, inputs2)\n",
    "#             loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_train_loss += loss.item() * inputs1.size(0)\n",
    "        \n",
    "#         # 평균 훈련 손실 계산\n",
    "#         epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "#         train_losses.append(epoch_train_loss)\n",
    "        \n",
    "#         model.eval()\n",
    "#         running_valid_loss = 0.0\n",
    "        \n",
    "#         # 검증 데이터로 모델 평가\n",
    "#         for inputs1, inputs2, labels in valid_loader:\n",
    "#             inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "#             outputs = model(inputs1, inputs2)\n",
    "#             loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "#             running_valid_loss += loss.item() * inputs1.size(0)\n",
    "        \n",
    "#         # 평균 검증 손실 계산\n",
    "#         epoch_valid_loss = running_valid_loss / len(valid_loader.dataset)\n",
    "#         valid_losses.append(epoch_valid_loss)\n",
    "        \n",
    "#         # 에포크마다 학습 결과 출력\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.8f}, Valid Loss: {epoch_valid_loss:.8f}')\n",
    "\n",
    "#     torch.save(model.state_dict(), 'Check_Model.pth')\n",
    "#     print(\"모델이 저장되었습니다.\")\n",
    "#     return train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d9168-46b7-49c7-8d29-5dc96d2c16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison 레이어: train\n",
    "\n",
    "# def train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs):\n",
    "#     train_losses = []\n",
    "#     valid_losses = []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_train_loss = 0.0\n",
    "        \n",
    "#         # 훈련 데이터로 모델 학습\n",
    "#         for inputs1, inputs2, labels in train_loader:\n",
    "#             inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs1, inputs2)\n",
    "#             loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_train_loss += loss.item() * inputs1.size(0)\n",
    "        \n",
    "#         # 평균 훈련 손실 계산\n",
    "#         epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "#         train_losses.append(epoch_train_loss)\n",
    "        \n",
    "#         model.eval()\n",
    "#         running_valid_loss = 0.0\n",
    "        \n",
    "#         # 검증 데이터로 모델 평가\n",
    "#         for inputs1, inputs2, labels in valid_loader:\n",
    "#             inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "#             outputs = model(inputs1, inputs2)\n",
    "#             loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "#             running_valid_loss += loss.item() * inputs1.size(0)\n",
    "        \n",
    "#         # 평균 검증 손실 계산\n",
    "#         epoch_valid_loss = running_valid_loss / len(valid_loader.dataset)\n",
    "#         valid_losses.append(epoch_valid_loss)\n",
    "        \n",
    "#         # 에포크마다 학습 결과 출력\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.8f}, Valid Loss: {epoch_valid_loss:.8f}')\n",
    "\n",
    "#     torch.save(model.state_dict(), 'Check_Model.pth')\n",
    "#     print(\"모델이 저장되었습니다.\")\n",
    "#     return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc12c10a-cf06-4a3c-a4db-f3bcdae0cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"M_check\"\n",
    "\n",
    "def save_model(model, save_path):\n",
    "    # 새로운 폴더 생성 (없으면)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # 현재 시간 가져오기\n",
    "    now = datetime.datetime.now()\n",
    "    # 모델 저장\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, f\"model_{now.strftime('%Y%m%d_%H%M%S')}.pth\"))\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_valid_loss = float('inf')\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        # 훈련 데이터로 모델 학습\n",
    "        for inputs1, inputs2, labels in train_loader:\n",
    "            inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs1, inputs2)\n",
    "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * inputs1.size(0)\n",
    "        \n",
    "        # 평균 훈련 손실 계산\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        running_valid_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # 검증 데이터로 모델 평가\n",
    "        for inputs1, inputs2, labels in valid_loader:\n",
    "            inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "            outputs = model(inputs1, inputs2)\n",
    "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            running_valid_loss += loss.item() * inputs1.size(0)\n",
    "            \n",
    "            # 정확도 계산\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            correct += (predicted == labels.view(-1, 1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        # 평균 검증 손실 계산\n",
    "        epoch_valid_loss = running_valid_loss / len(valid_loader.dataset)\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "        \n",
    "        # 정확도 계산\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # 에포크마다 학습 결과 출력\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.8f}, Valid Loss: {epoch_valid_loss:.8f}, Accuracy: {accuracy:.8f}')\n",
    "        \n",
    "        # 더 나은 모델인 경우 저장\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            if save_path:\n",
    "                save_model(model, save_path)\n",
    "        \n",
    "        # 모델의 손실이 더 낮아졌을 때 저장\n",
    "        if epoch_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = epoch_valid_loss\n",
    "            if save_path:\n",
    "                save_model(model, save_path)\n",
    "\n",
    "    print(\"모델이 저장되었습니다.\")\n",
    "    return train_losses, valid_losses, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "851e7a9d-a5e4-41a8-b7b9-334c797cdcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.69332522, Valid Loss: 0.69314975, Accuracy: 0.50000000\n",
      "Epoch 2/50, Train Loss: 0.69331849, Valid Loss: 0.69315849, Accuracy: 0.50000000\n",
      "Epoch 3/50, Train Loss: 0.69328434, Valid Loss: 0.69316806, Accuracy: 0.50000000\n",
      "Epoch 4/50, Train Loss: 0.69327813, Valid Loss: 0.69314754, Accuracy: 0.50000000\n",
      "Epoch 5/50, Train Loss: 0.69325082, Valid Loss: 0.69314725, Accuracy: 0.50000000\n",
      "Epoch 6/50, Train Loss: 0.69327386, Valid Loss: 0.69314943, Accuracy: 0.50000000\n",
      "Epoch 7/50, Train Loss: 0.69326459, Valid Loss: 0.69316250, Accuracy: 0.50000000\n",
      "Epoch 8/50, Train Loss: 0.69319084, Valid Loss: 0.69316243, Accuracy: 0.50000000\n",
      "Epoch 9/50, Train Loss: 0.69325080, Valid Loss: 0.69314993, Accuracy: 0.50000000\n",
      "Epoch 10/50, Train Loss: 0.69320882, Valid Loss: 0.69315726, Accuracy: 0.50000000\n",
      "Epoch 11/50, Train Loss: 0.69322731, Valid Loss: 0.69315314, Accuracy: 0.50000000\n",
      "Epoch 12/50, Train Loss: 0.69321196, Valid Loss: 0.69315565, Accuracy: 0.50000000\n",
      "Epoch 13/50, Train Loss: 0.69323610, Valid Loss: 0.69314802, Accuracy: 0.50000000\n",
      "Epoch 14/50, Train Loss: 0.69322727, Valid Loss: 0.69314885, Accuracy: 0.50000000\n",
      "Epoch 15/50, Train Loss: 0.69322496, Valid Loss: 0.69315412, Accuracy: 0.50000000\n",
      "Epoch 16/50, Train Loss: 0.69322102, Valid Loss: 0.69314813, Accuracy: 0.50000000\n",
      "Epoch 17/50, Train Loss: 0.69316864, Valid Loss: 0.69323480, Accuracy: 0.50000000\n",
      "Epoch 18/50, Train Loss: 0.69328267, Valid Loss: 0.69315112, Accuracy: 0.50000000\n",
      "Epoch 19/50, Train Loss: 0.69319101, Valid Loss: 0.69314825, Accuracy: 0.50000000\n",
      "Epoch 20/50, Train Loss: 0.69319505, Valid Loss: 0.69314939, Accuracy: 0.50000000\n",
      "Epoch 21/50, Train Loss: 0.69320206, Valid Loss: 0.69314883, Accuracy: 0.50000000\n",
      "Epoch 22/50, Train Loss: 0.69320803, Valid Loss: 0.69314814, Accuracy: 0.50000000\n",
      "Epoch 23/50, Train Loss: 0.69319815, Valid Loss: 0.69315564, Accuracy: 0.50000000\n",
      "Epoch 24/50, Train Loss: 0.69319638, Valid Loss: 0.69315408, Accuracy: 0.50000000\n",
      "Epoch 25/50, Train Loss: 0.69319933, Valid Loss: 0.69314731, Accuracy: 0.50000000\n",
      "Epoch 26/50, Train Loss: 0.69319569, Valid Loss: 0.69314790, Accuracy: 0.50000000\n",
      "Epoch 27/50, Train Loss: 0.69319642, Valid Loss: 0.69314903, Accuracy: 0.50000000\n",
      "Epoch 28/50, Train Loss: 0.69321020, Valid Loss: 0.69314993, Accuracy: 0.50000000\n",
      "Epoch 29/50, Train Loss: 0.69320486, Valid Loss: 0.69314843, Accuracy: 0.50000000\n",
      "Epoch 30/50, Train Loss: 0.69320739, Valid Loss: 0.69314742, Accuracy: 0.50000000\n",
      "Epoch 31/50, Train Loss: 0.69319164, Valid Loss: 0.69314765, Accuracy: 0.50000000\n",
      "Epoch 32/50, Train Loss: 0.69320024, Valid Loss: 0.69315100, Accuracy: 0.50000000\n",
      "Epoch 33/50, Train Loss: 0.69321669, Valid Loss: 0.69314724, Accuracy: 0.50000000\n",
      "Epoch 34/50, Train Loss: 0.69318576, Valid Loss: 0.69314820, Accuracy: 0.50000000\n",
      "Epoch 35/50, Train Loss: 0.69319903, Valid Loss: 0.69314884, Accuracy: 0.50000000\n",
      "Epoch 36/50, Train Loss: 0.69322226, Valid Loss: 0.69314718, Accuracy: 0.50000000\n",
      "Epoch 37/50, Train Loss: 0.69318589, Valid Loss: 0.69314795, Accuracy: 0.50000000\n",
      "Epoch 38/50, Train Loss: 0.69321037, Valid Loss: 0.69314744, Accuracy: 0.50000000\n",
      "Epoch 39/50, Train Loss: 0.69322749, Valid Loss: 0.69314730, Accuracy: 0.50000000\n",
      "Epoch 40/50, Train Loss: 0.69320213, Valid Loss: 0.69314754, Accuracy: 0.50000000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train/chemdraw_train_smlies/US07314887-20080101-C00215.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m running_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 훈련 데이터로 모델 학습\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs1, inputs2, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     23\u001b[0m     inputs1, inputs2, labels \u001b[38;5;241m=\u001b[39m inputs1\u001b[38;5;241m.\u001b[39mto(device), inputs2\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/molscribe/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/molscribe/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/molscribe/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/molscribe/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mCombinedImagePairDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset1):\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset1\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Adjust the index to fit within the second dataset\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         adjusted_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset1)\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36mImagePairDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m img1_path, img2_path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n\u001b[1;32m     22\u001b[0m img1 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img1_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m img2 \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg2_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     26\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img1)\n",
      "File \u001b[0;32m~/miniconda3/envs/molscribe/lib/python3.9/site-packages/PIL/Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train/chemdraw_train_smlies/US07314887-20080101-C00215.png'"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "train_losses, valid_losses = train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs=num_epochs, save_path = save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70880ab5-8bbf-473e-9840-6be9aa7c459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 시각화\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Valid Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398013a6-b752-4a1a-b2fd-9690307f123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_improve = 5\n",
    "\n",
    "# 모델 개선 함수\n",
    "def improve_model(model, optimizer, criterion, train_loader, valid_loader, num_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_valid_loss += loss.item() * inputs.size(0)\n",
    "            epoch_valid_loss = running_valid_loss / len(valid_loader.dataset)\n",
    "            valid_losses.append(epoch_valid_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Valid Loss: {epoch_valid_loss:.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'improved_model.pth')\n",
    "    print(\"모델이 저장되었습니다.\")\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75e2b7-f502-471b-a309-973ec6be7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 개선 및 재학습\n",
    "improved_train_losses, improved_valid_losses = improve_model(model, optimizer, criterion, train_loader, valid_loader, num_epochs_improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c0e8b-8587-4804-8987-057369ed3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion, test_loader):\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs1, inputs2 in test_loader:\n",
    "            inputs1, inputs2 = inputs1.to(device), inputs2.to(device)\n",
    "            outputs = model(inputs1, inputs2)\n",
    "            loss = criterion(outputs, torch.ones(outputs.shape[0], 1, device=device))  # 이진 분류 문제에서는 일관된 레이블 사용\n",
    "            total_loss += loss.item() * inputs1.size(0)\n",
    "            \n",
    "            # 모델의 예측 결과를 확인하여 정확도 계산\n",
    "            predicted_labels = torch.round(torch.sigmoid(outputs))\n",
    "            correct_predictions += (predicted_labels == 1).sum().item()\n",
    "            \n",
    "            total_samples += inputs1.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    \n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092df361-7b7f-44c5-8897-9378eb670855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋에 대해 모델 평가\n",
    "test_loss, test_accuracy = evaluate_model(model, criterion, test_loader)\n",
    "print(f'Test Loss: {test_loss:.8f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a43ef8-4c5b-4816-a4f4-6cc79722cb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
